---------- Begin SLURM Prolog ----------
Job ID:        7207207
Username:      siliangc
Accountname:   lc_fs3
Name:          batchrun.sh
Partition:     main
Nodelist:      hpc4669
TasksPerNode:  1
CPUsPerTask:   Default[1]
TMPDIR:        /tmp/7207207.main
SCRATCHDIR:    /staging/scratch/7207207
Cluster:       uschpc
HSDA Account:  false
---------- 2020-05-05 13:37:50 ---------
2020-05-05 13:49:20.160037: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1
2020-05-05 13:49:20.294508: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: 
pciBusID: 0000:3b:00.0 name: Tesla V100-PCIE-32GB computeCapability: 7.0
coreClock: 1.38GHz coreCount: 80 deviceMemorySize: 31.72GiB deviceMemoryBandwidth: 836.37GiB/s
2020-05-05 13:49:20.296814: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 1 with properties: 
pciBusID: 0000:d8:00.0 name: Tesla V100-PCIE-32GB computeCapability: 7.0
coreClock: 1.38GHz coreCount: 80 deviceMemorySize: 31.72GiB deviceMemoryBandwidth: 836.37GiB/s
2020-05-05 13:49:20.354554: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
2020-05-05 13:49:21.047358: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
2020-05-05 13:49:21.408129: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10
2020-05-05 13:49:21.904597: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10
2020-05-05 13:49:22.251549: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10
2020-05-05 13:49:22.406654: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10
2020-05-05 13:49:22.902697: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-05-05 13:49:22.913519: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0, 1
2020-05-05 13:49:22.916649: I tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA
2020-05-05 13:49:23.196933: I tensorflow/core/platform/profile_utils/cpu_utils.cc:102] CPU Frequency: 2100000000 Hz
2020-05-05 13:49:23.201583: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5615381d5ff0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-05-05 13:49:23.201647: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2020-05-05 13:49:23.640587: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x5615382401a0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2020-05-05 13:49:23.640663: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-PCIE-32GB, Compute Capability 7.0
2020-05-05 13:49:23.640692: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (1): Tesla V100-PCIE-32GB, Compute Capability 7.0
2020-05-05 13:49:23.673058: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: 
pciBusID: 0000:3b:00.0 name: Tesla V100-PCIE-32GB computeCapability: 7.0
coreClock: 1.38GHz coreCount: 80 deviceMemorySize: 31.72GiB deviceMemoryBandwidth: 836.37GiB/s
2020-05-05 13:49:23.680222: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 1 with properties: 
pciBusID: 0000:d8:00.0 name: Tesla V100-PCIE-32GB computeCapability: 7.0
coreClock: 1.38GHz coreCount: 80 deviceMemorySize: 31.72GiB deviceMemoryBandwidth: 836.37GiB/s
2020-05-05 13:49:23.680292: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
2020-05-05 13:49:23.680329: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
2020-05-05 13:49:23.680349: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10
2020-05-05 13:49:23.680364: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10
2020-05-05 13:49:23.680378: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10
2020-05-05 13:49:23.680392: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10
2020-05-05 13:49:23.680407: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-05-05 13:49:23.687815: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0, 1
2020-05-05 13:49:23.687873: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
2020-05-05 13:49:23.691327: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-05-05 13:49:23.691347: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1108]      0 1 
2020-05-05 13:49:23.691356: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1121] 0:   N Y 
2020-05-05 13:49:23.691363: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1121] 1:   Y N 
2020-05-05 13:49:23.701081: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1247] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 30233 MB memory) -> physical GPU (device: 0, name: Tesla V100-PCIE-32GB, pci bus id: 0000:3b:00.0, compute capability: 7.0)
2020-05-05 13:49:23.703069: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1247] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 30233 MB memory) -> physical GPU (device: 1, name: Tesla V100-PCIE-32GB, pci bus id: 0000:d8:00.0, compute capability: 7.0)
2020-05-05 13:49:43.181125: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 14599008000 exceeds 10% of free system memory.
2020-05-05 13:50:16.194192: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 14599008000 exceeds 10% of free system memory.
2020-05-05 13:50:28.254053: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
2020-05-05 13:50:29.104044: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
Processing Data...
Processing sequences of length: 3kk

Processing forward host data...
Processing prokaryote training data from ProkTr#ProkTr#3k_num1_seq66847_codefw.npy
Processing eukaryote training data from EukTr#EukTr#3k_num1_seq65382_codefw.npy
Processing reverse host data...
Processing prokaryote training data from ProkTr#ProkTr#3k_num1_seq66847_codebw.npy
Processing eukaryote training data from EukTr#EukTr#3k_num1_seq65382_codebw.npy
Processing plasmid data
Processing plasmid data - bw
Processing virus data...
Processing prokaryotevirus training data from ProkVirusTr#ProkVirusTr#3k_num1_seq66765_codefw.npy
Processin eukaryotevirus training data from EukVirusTr#EukVirusTr#3k_num1_seq34669_codefw.npy
Processing virus data...
Processing prokaryotevirus training data from ProkVirusTr#ProkVirusTr#3k_num1_seq66765_codebw.npy
Processin eukaryotevirus training data from EukVirusTr#EukVirusTr#3k_num1_seq34669_codebw.npy
Processing Validation Data...
Processing Validation Data - bw...
(304146, 3000, 4)
(304146, 3000, 4)
(304146, 5)
(32442, 3000, 4)
(32442, 3000, 4)
(32442, 5)
Model: "model"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            [(None, None, 4)]    0                                            
__________________________________________________________________________________________________
input_2 (InputLayer)            [(None, None, 4)]    0                                            
__________________________________________________________________________________________________
conv1d (Conv1D)                 (None, None, 64)     1600        input_1[0][0]                    
                                                                 input_2[0][0]                    
__________________________________________________________________________________________________
max_pooling1d (MaxPooling1D)    (None, None, 64)     0           conv1d[0][0]                     
                                                                 conv1d[1][0]                     
__________________________________________________________________________________________________
batch_normalization (BatchNorma (None, None, 64)     256         max_pooling1d[0][0]              
                                                                 max_pooling1d[1][0]              
__________________________________________________________________________________________________
conv1d_1 (Conv1D)               (None, None, 128)    24704       batch_normalization[0][0]        
                                                                 batch_normalization[1][0]        
__________________________________________________________________________________________________
max_pooling1d_1 (MaxPooling1D)  (None, None, 128)    0           conv1d_1[0][0]                   
                                                                 conv1d_1[1][0]                   
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, None, 128)    512         max_pooling1d_1[0][0]            
                                                                 max_pooling1d_1[1][0]            
__________________________________________________________________________________________________
conv1d_2 (Conv1D)               (None, None, 256)    65792       batch_normalization_1[0][0]      
                                                                 batch_normalization_1[1][0]      
__________________________________________________________________________________________________
global_max_pooling1d (GlobalMax (None, 256)          0           conv1d_2[0][0]                   
                                                                 conv1d_2[1][0]                   
__________________________________________________________________________________________________
dropout (Dropout)               (None, 256)          0           global_max_pooling1d[0][0]       
                                                                 global_max_pooling1d[1][0]       
__________________________________________________________________________________________________
flatten (Flatten)               (None, 256)          0           dropout[0][0]                    
                                                                 dropout[1][0]                    
__________________________________________________________________________________________________
dense (Dense)                   (None, 500)          128500      flatten[0][0]                    
                                                                 flatten[1][0]                    
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 500)          0           dense[0][0]                      
                                                                 dense[1][0]                      
__________________________________________________________________________________________________
dense_1 (Dense)                 (None, 5)            2505        dropout_1[0][0]                  
                                                                 dropout_1[1][0]                  
__________________________________________________________________________________________________
average (Average)               (None, 5)            0           dense_1[0][0]                    
                                                                 dense_1[1][0]                    
==================================================================================================
Total params: 223,869
Trainable params: 223,485
Non-trainable params: 384
__________________________________________________________________________________________________
Epoch 1/60

Epoch 00001: val_loss improved from inf to 0.69840, saving model to ./models/model_DNA_one-hot.h5
3042/3042 - 168s - loss: 0.8934 - accuracy: 0.6282 - val_loss: 0.6984 - val_accuracy: 0.7438
Epoch 2/60

Epoch 00002: val_loss did not improve from 0.69840
3042/3042 - 167s - loss: 0.6011 - accuracy: 0.7623 - val_loss: 0.7790 - val_accuracy: 0.7643
Epoch 3/60

Epoch 00003: val_loss did not improve from 0.69840
3042/3042 - 168s - loss: 0.5186 - accuracy: 0.8021 - val_loss: 0.7971 - val_accuracy: 0.7821
Epoch 4/60

Epoch 00004: val_loss did not improve from 0.69840
3042/3042 - 167s - loss: 0.4735 - accuracy: 0.8229 - val_loss: 0.9021 - val_accuracy: 0.7870
Epoch 5/60

Epoch 00005: val_loss did not improve from 0.69840
3042/3042 - 167s - loss: 0.4483 - accuracy: 0.8325 - val_loss: 0.9144 - val_accuracy: 0.8072
Epoch 6/60

Epoch 00006: val_loss did not improve from 0.69840
3042/3042 - 168s - loss: 0.4261 - accuracy: 0.8427 - val_loss: 0.7145 - val_accuracy: 0.7971
Epoch 7/60

Epoch 00007: val_loss did not improve from 0.69840
3042/3042 - 170s - loss: 0.4105 - accuracy: 0.8487 - val_loss: 0.8252 - val_accuracy: 0.8044
Epoch 8/60

Epoch 00008: val_loss did not improve from 0.69840
3042/3042 - 167s - loss: 0.3958 - accuracy: 0.8554 - val_loss: 0.7429 - val_accuracy: 0.8135
Epoch 9/60

Epoch 00009: val_loss did not improve from 0.69840
3042/3042 - 171s - loss: 0.3855 - accuracy: 0.8596 - val_loss: 0.8950 - val_accuracy: 0.8157
Epoch 10/60

Epoch 00010: val_loss did not improve from 0.69840
3042/3042 - 168s - loss: 0.3756 - accuracy: 0.8642 - val_loss: 0.8586 - val_accuracy: 0.8174
Epoch 11/60

Epoch 00011: val_loss did not improve from 0.69840
3042/3042 - 169s - loss: 0.3658 - accuracy: 0.8679 - val_loss: 0.8978 - val_accuracy: 0.8164
Epoch 12/60

Epoch 00012: val_loss did not improve from 0.69840
3042/3042 - 169s - loss: 0.3576 - accuracy: 0.8710 - val_loss: 0.7888 - val_accuracy: 0.8201
Epoch 13/60

Epoch 00013: val_loss did not improve from 0.69840
3042/3042 - 169s - loss: 0.3513 - accuracy: 0.8736 - val_loss: 0.8033 - val_accuracy: 0.8224
Epoch 14/60

Epoch 00014: val_loss did not improve from 0.69840
3042/3042 - 170s - loss: 0.3439 - accuracy: 0.8764 - val_loss: 0.8493 - val_accuracy: 0.8241
Epoch 15/60

Epoch 00015: val_loss did not improve from 0.69840
3042/3042 - 171s - loss: 0.3371 - accuracy: 0.8785 - val_loss: 0.8034 - val_accuracy: 0.8223
Epoch 16/60

Epoch 00016: val_loss did not improve from 0.69840
3042/3042 - 172s - loss: 0.3338 - accuracy: 0.8801 - val_loss: 0.9063 - val_accuracy: 0.8284
Epoch 17/60

Epoch 00017: val_loss did not improve from 0.69840
3042/3042 - 166s - loss: 0.3265 - accuracy: 0.8828 - val_loss: 0.8109 - val_accuracy: 0.8261
Epoch 18/60

Epoch 00018: val_loss did not improve from 0.69840
3042/3042 - 167s - loss: 0.3226 - accuracy: 0.8846 - val_loss: 1.0100 - val_accuracy: 0.8262
Epoch 19/60

Epoch 00019: val_loss did not improve from 0.69840
3042/3042 - 167s - loss: 0.3187 - accuracy: 0.8861 - val_loss: 0.7602 - val_accuracy: 0.8339
Epoch 20/60

Epoch 00020: val_loss did not improve from 0.69840
3042/3042 - 169s - loss: 0.3140 - accuracy: 0.8879 - val_loss: 0.7736 - val_accuracy: 0.8196
Epoch 21/60

Epoch 00021: val_loss did not improve from 0.69840
3042/3042 - 170s - loss: 0.3107 - accuracy: 0.8893 - val_loss: 0.8024 - val_accuracy: 0.8306
Epoch 22/60

Epoch 00022: val_loss did not improve from 0.69840
3042/3042 - 169s - loss: 0.3068 - accuracy: 0.8910 - val_loss: 0.7610 - val_accuracy: 0.8386
Epoch 23/60

Epoch 00023: val_loss did not improve from 0.69840
3042/3042 - 169s - loss: 0.3045 - accuracy: 0.8914 - val_loss: 0.9231 - val_accuracy: 0.8328
Epoch 24/60

Epoch 00024: val_loss did not improve from 0.69840
3042/3042 - 169s - loss: 0.2992 - accuracy: 0.8945 - val_loss: 0.7736 - val_accuracy: 0.8313
Epoch 25/60

Epoch 00025: val_loss did not improve from 0.69840
3042/3042 - 170s - loss: 0.2979 - accuracy: 0.8948 - val_loss: 0.9047 - val_accuracy: 0.8358
Epoch 26/60

Epoch 00026: val_loss did not improve from 0.69840
3042/3042 - 167s - loss: 0.2926 - accuracy: 0.8959 - val_loss: 0.9380 - val_accuracy: 0.8361
Epoch 27/60

Epoch 00027: val_loss did not improve from 0.69840
3042/3042 - 169s - loss: 0.2914 - accuracy: 0.8971 - val_loss: 0.9112 - val_accuracy: 0.8287
Epoch 28/60

Epoch 00028: val_loss did not improve from 0.69840
3042/3042 - 167s - loss: 0.2893 - accuracy: 0.8977 - val_loss: 0.8375 - val_accuracy: 0.8245
Epoch 29/60

Epoch 00029: val_loss did not improve from 0.69840
3042/3042 - 167s - loss: 0.2855 - accuracy: 0.8989 - val_loss: 0.9305 - val_accuracy: 0.8289
Epoch 30/60

Epoch 00030: val_loss did not improve from 0.69840
3042/3042 - 169s - loss: 0.2820 - accuracy: 0.9006 - val_loss: 0.7391 - val_accuracy: 0.8390
Epoch 31/60

Epoch 00031: val_loss did not improve from 0.69840
3042/3042 - 168s - loss: 0.2808 - accuracy: 0.9013 - val_loss: 0.7798 - val_accuracy: 0.8365
Epoch 32/60

Epoch 00032: val_loss did not improve from 0.69840
3042/3042 - 170s - loss: 0.2781 - accuracy: 0.9030 - val_loss: 0.9347 - val_accuracy: 0.8295
Epoch 33/60

Epoch 00033: val_loss did not improve from 0.69840
3042/3042 - 170s - loss: 0.2774 - accuracy: 0.9022 - val_loss: 0.8177 - val_accuracy: 0.8351
Epoch 34/60

Epoch 00034: val_loss did not improve from 0.69840
3042/3042 - 167s - loss: 0.2736 - accuracy: 0.9039 - val_loss: 0.9819 - val_accuracy: 0.8315
Epoch 35/60

Epoch 00035: val_loss did not improve from 0.69840
3042/3042 - 167s - loss: 0.2715 - accuracy: 0.9046 - val_loss: 0.8903 - val_accuracy: 0.8370
Epoch 36/60

Epoch 00036: val_loss did not improve from 0.69840
3042/3042 - 169s - loss: 0.2697 - accuracy: 0.9051 - val_loss: 0.9572 - val_accuracy: 0.8347
Epoch 37/60

Epoch 00037: val_loss did not improve from 0.69840
3042/3042 - 168s - loss: 0.2672 - accuracy: 0.9066 - val_loss: 0.9001 - val_accuracy: 0.8337
Epoch 38/60

Epoch 00038: val_loss did not improve from 0.69840
3042/3042 - 169s - loss: 0.2645 - accuracy: 0.9076 - val_loss: 0.9156 - val_accuracy: 0.8328
Epoch 39/60

Epoch 00039: val_loss did not improve from 0.69840
3042/3042 - 172s - loss: 0.2637 - accuracy: 0.9083 - val_loss: 0.9323 - val_accuracy: 0.8332
Epoch 40/60

Epoch 00040: val_loss did not improve from 0.69840
3042/3042 - 169s - loss: 0.2631 - accuracy: 0.9077 - val_loss: 0.8720 - val_accuracy: 0.8383
Epoch 00040: early stopping
