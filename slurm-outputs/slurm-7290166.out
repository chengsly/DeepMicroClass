---------- Begin SLURM Prolog ----------
Job ID:        7290166
Username:      siliangc
Accountname:   lc_fs3
Name:          batchrun.sh
Partition:     main
Nodelist:      hpc4571
TasksPerNode:  1
CPUsPerTask:   Default[1]
TMPDIR:        /tmp/7290166.main
SCRATCHDIR:    /staging/scratch/7290166
Cluster:       uschpc
HSDA Account:  false
---------- 2020-05-07 13:36:21 ---------
2020-05-07 13:49:15.131656: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1
2020-05-07 13:49:15.269621: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: 
pciBusID: 0000:3b:00.0 name: Tesla V100-PCIE-32GB computeCapability: 7.0
coreClock: 1.38GHz coreCount: 80 deviceMemorySize: 31.72GiB deviceMemoryBandwidth: 836.37GiB/s
2020-05-07 13:49:15.270882: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 1 with properties: 
pciBusID: 0000:d8:00.0 name: Tesla V100-PCIE-32GB computeCapability: 7.0
coreClock: 1.38GHz coreCount: 80 deviceMemorySize: 31.72GiB deviceMemoryBandwidth: 836.37GiB/s
2020-05-07 13:49:15.303295: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
2020-05-07 13:49:15.368248: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
2020-05-07 13:49:15.404299: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10
2020-05-07 13:49:15.437185: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10
2020-05-07 13:49:15.473640: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10
2020-05-07 13:49:15.507517: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10
2020-05-07 13:49:15.551374: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-05-07 13:49:15.560313: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0, 1
2020-05-07 13:49:15.560998: I tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA
2020-05-07 13:49:15.578312: I tensorflow/core/platform/profile_utils/cpu_utils.cc:102] CPU Frequency: 2100000000 Hz
2020-05-07 13:49:15.581196: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x557519f3d3c0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-05-07 13:49:15.581227: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2020-05-07 13:49:15.948867: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x557519fa7570 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2020-05-07 13:49:15.948948: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-PCIE-32GB, Compute Capability 7.0
2020-05-07 13:49:15.948976: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (1): Tesla V100-PCIE-32GB, Compute Capability 7.0
2020-05-07 13:49:15.959396: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: 
pciBusID: 0000:3b:00.0 name: Tesla V100-PCIE-32GB computeCapability: 7.0
coreClock: 1.38GHz coreCount: 80 deviceMemorySize: 31.72GiB deviceMemoryBandwidth: 836.37GiB/s
2020-05-07 13:49:15.961574: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 1 with properties: 
pciBusID: 0000:d8:00.0 name: Tesla V100-PCIE-32GB computeCapability: 7.0
coreClock: 1.38GHz coreCount: 80 deviceMemorySize: 31.72GiB deviceMemoryBandwidth: 836.37GiB/s
2020-05-07 13:49:15.961642: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
2020-05-07 13:49:15.961664: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
2020-05-07 13:49:15.961691: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10
2020-05-07 13:49:15.961712: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10
2020-05-07 13:49:15.961731: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10
2020-05-07 13:49:15.961751: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10
2020-05-07 13:49:15.961777: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-05-07 13:49:15.968776: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0, 1
2020-05-07 13:49:15.968814: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
2020-05-07 13:49:15.972089: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-05-07 13:49:15.972109: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1108]      0 1 
2020-05-07 13:49:15.972116: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1121] 0:   N Y 
2020-05-07 13:49:15.972122: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1121] 1:   Y N 
2020-05-07 13:49:15.976987: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1247] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 30233 MB memory) -> physical GPU (device: 0, name: Tesla V100-PCIE-32GB, pci bus id: 0000:3b:00.0, compute capability: 7.0)
2020-05-07 13:49:15.978779: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1247] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 30233 MB memory) -> physical GPU (device: 1, name: Tesla V100-PCIE-32GB, pci bus id: 0000:d8:00.0, compute capability: 7.0)
2020-05-07 13:49:24.017685: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 14599008000 exceeds 10% of free system memory.
2020-05-07 13:49:56.794255: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 14599008000 exceeds 10% of free system memory.
2020-05-07 13:50:22.001734: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
2020-05-07 13:50:22.448940: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
Processing Data...
Processing sequences of length: 3kk

Processing forward host data...
Processing prokaryote training data from ProkTr#ProkTr#3k_num1_seq66847_codefw.npy
Processing eukaryote training data from EukTr#EukTr#3k_num1_seq65382_codefw.npy
Processing reverse host data...
Processing prokaryote training data from ProkTr#ProkTr#3k_num1_seq66847_codebw.npy
Processing eukaryote training data from EukTr#EukTr#3k_num1_seq65382_codebw.npy
Processing plasmid data
Processing plasmid data - bw
Processing virus data...
Processing prokaryotevirus training data from ProkVirusTr#ProkVirusTr#3k_num1_seq66765_codefw.npy
Processin eukaryotevirus training data from EukVirusTr#EukVirusTr#3k_num1_seq34669_codefw.npy
Processing virus data...
Processing prokaryotevirus training data from ProkVirusTr#ProkVirusTr#3k_num1_seq66765_codebw.npy
Processin eukaryotevirus training data from EukVirusTr#EukVirusTr#3k_num1_seq34669_codebw.npy
Processing Validation Data...
Processing Validation Data - bw...
(304146, 3000, 4)
(304146, 3000, 4)
(304146, 5)
(32442, 3000, 4)
(32442, 3000, 4)
(32442, 5)
Model: "model"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            [(None, None, 4)]    0                                            
__________________________________________________________________________________________________
input_2 (InputLayer)            [(None, None, 4)]    0                                            
__________________________________________________________________________________________________
conv1d (Conv1D)                 (None, None, 64)     2112        input_1[0][0]                    
                                                                 input_2[0][0]                    
__________________________________________________________________________________________________
max_pooling1d (MaxPooling1D)    (None, None, 64)     0           conv1d[0][0]                     
                                                                 conv1d[1][0]                     
__________________________________________________________________________________________________
conv1d_1 (Conv1D)               (None, None, 64)     24640       max_pooling1d[0][0]              
                                                                 max_pooling1d[1][0]              
__________________________________________________________________________________________________
max_pooling1d_1 (MaxPooling1D)  (None, None, 64)     0           conv1d_1[0][0]                   
                                                                 conv1d_1[1][0]                   
__________________________________________________________________________________________________
batch_normalization (BatchNorma (None, None, 64)     256         max_pooling1d_1[0][0]            
                                                                 max_pooling1d_1[1][0]            
__________________________________________________________________________________________________
conv1d_2 (Conv1D)               (None, None, 128)    24704       batch_normalization[0][0]        
                                                                 batch_normalization[1][0]        
__________________________________________________________________________________________________
max_pooling1d_2 (MaxPooling1D)  (None, None, 128)    0           conv1d_2[0][0]                   
                                                                 conv1d_2[1][0]                   
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, None, 128)    512         max_pooling1d_2[0][0]            
                                                                 max_pooling1d_2[1][0]            
__________________________________________________________________________________________________
conv1d_3 (Conv1D)               (None, None, 256)    65792       batch_normalization_1[0][0]      
                                                                 batch_normalization_1[1][0]      
__________________________________________________________________________________________________
global_max_pooling1d (GlobalMax (None, 256)          0           conv1d_3[0][0]                   
                                                                 conv1d_3[1][0]                   
__________________________________________________________________________________________________
dropout (Dropout)               (None, 256)          0           global_max_pooling1d[0][0]       
                                                                 global_max_pooling1d[1][0]       
__________________________________________________________________________________________________
flatten (Flatten)               (None, 256)          0           dropout[0][0]                    
                                                                 dropout[1][0]                    
__________________________________________________________________________________________________
dense (Dense)                   (None, 500)          128500      flatten[0][0]                    
                                                                 flatten[1][0]                    
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 500)          0           dense[0][0]                      
                                                                 dense[1][0]                      
__________________________________________________________________________________________________
dense_1 (Dense)                 (None, 5)            2505        dropout_1[0][0]                  
                                                                 dropout_1[1][0]                  
__________________________________________________________________________________________________
average (Average)               (None, 5)            0           dense_1[0][0]                    
                                                                 dense_1[1][0]                    
==================================================================================================
Total params: 249,021
Trainable params: 248,637
Non-trainable params: 384
__________________________________________________________________________________________________
Epoch 1/60

Epoch 00001: val_loss improved from inf to 0.91239, saving model to ./models/model_DNA_one-hot_3000.h5
3042/3042 - 141s - loss: 0.8992 - accuracy: 0.6243 - val_loss: 0.9124 - val_accuracy: 0.5988
Epoch 2/60

Epoch 00002: val_loss improved from 0.91239 to 0.71297, saving model to ./models/model_DNA_one-hot_3000.h5
3042/3042 - 134s - loss: 0.6531 - accuracy: 0.7390 - val_loss: 0.7130 - val_accuracy: 0.7430
Epoch 3/60

Epoch 00003: val_loss did not improve from 0.71297
3042/3042 - 132s - loss: 0.5513 - accuracy: 0.7865 - val_loss: 0.9474 - val_accuracy: 0.7090
Epoch 4/60

Epoch 00004: val_loss did not improve from 0.71297
3042/3042 - 133s - loss: 0.5024 - accuracy: 0.8069 - val_loss: 0.9534 - val_accuracy: 0.7382
Epoch 5/60

Epoch 00005: val_loss did not improve from 0.71297
3042/3042 - 133s - loss: 0.4675 - accuracy: 0.8227 - val_loss: 0.7297 - val_accuracy: 0.7765
Epoch 6/60

Epoch 00006: val_loss did not improve from 0.71297
3042/3042 - 134s - loss: 0.4438 - accuracy: 0.8338 - val_loss: 0.8242 - val_accuracy: 0.7198
Epoch 7/60

Epoch 00007: val_loss improved from 0.71297 to 0.70815, saving model to ./models/model_DNA_one-hot_3000.h5
3042/3042 - 133s - loss: 0.4222 - accuracy: 0.8422 - val_loss: 0.7081 - val_accuracy: 0.7875
Epoch 8/60

Epoch 00008: val_loss did not improve from 0.70815
3042/3042 - 134s - loss: 0.4091 - accuracy: 0.8475 - val_loss: 0.8159 - val_accuracy: 0.7598
Epoch 9/60

Epoch 00009: val_loss improved from 0.70815 to 0.63761, saving model to ./models/model_DNA_one-hot_3000.h5
3042/3042 - 134s - loss: 0.3937 - accuracy: 0.8539 - val_loss: 0.6376 - val_accuracy: 0.7945
Epoch 10/60

Epoch 00010: val_loss did not improve from 0.63761
3042/3042 - 134s - loss: 0.3822 - accuracy: 0.8585 - val_loss: 0.6668 - val_accuracy: 0.7966
Epoch 11/60

Epoch 00011: val_loss did not improve from 0.63761
3042/3042 - 132s - loss: 0.3721 - accuracy: 0.8626 - val_loss: 0.7805 - val_accuracy: 0.7866
Epoch 12/60

Epoch 00012: val_loss did not improve from 0.63761
3042/3042 - 133s - loss: 0.3616 - accuracy: 0.8667 - val_loss: 0.7071 - val_accuracy: 0.8252
Epoch 13/60

Epoch 00013: val_loss did not improve from 0.63761
3042/3042 - 135s - loss: 0.3533 - accuracy: 0.8701 - val_loss: 0.6905 - val_accuracy: 0.7768
Epoch 14/60

Epoch 00014: val_loss did not improve from 0.63761
3042/3042 - 135s - loss: 0.3456 - accuracy: 0.8729 - val_loss: 0.6669 - val_accuracy: 0.8081
Epoch 15/60

Epoch 00015: val_loss did not improve from 0.63761
3042/3042 - 133s - loss: 0.3373 - accuracy: 0.8767 - val_loss: 0.9333 - val_accuracy: 0.7585
Epoch 16/60

Epoch 00016: val_loss did not improve from 0.63761
3042/3042 - 133s - loss: 0.3320 - accuracy: 0.8787 - val_loss: 0.6561 - val_accuracy: 0.8235
Epoch 17/60

Epoch 00017: val_loss did not improve from 0.63761
3042/3042 - 132s - loss: 0.3247 - accuracy: 0.8818 - val_loss: 0.7030 - val_accuracy: 0.8251
Epoch 18/60

Epoch 00018: val_loss did not improve from 0.63761
3042/3042 - 134s - loss: 0.3198 - accuracy: 0.8838 - val_loss: 0.8510 - val_accuracy: 0.8193
Epoch 19/60

Epoch 00019: val_loss did not improve from 0.63761
3042/3042 - 133s - loss: 0.3151 - accuracy: 0.8857 - val_loss: 0.8506 - val_accuracy: 0.8242
Epoch 20/60

Epoch 00020: val_loss did not improve from 0.63761
3042/3042 - 131s - loss: 0.3100 - accuracy: 0.8876 - val_loss: 0.7899 - val_accuracy: 0.8240
Epoch 21/60

Epoch 00021: val_loss did not improve from 0.63761
3042/3042 - 132s - loss: 0.3048 - accuracy: 0.8900 - val_loss: 0.7004 - val_accuracy: 0.8281
Epoch 22/60

Epoch 00022: val_loss did not improve from 0.63761
3042/3042 - 133s - loss: 0.2995 - accuracy: 0.8919 - val_loss: 0.6420 - val_accuracy: 0.8368
Epoch 23/60

Epoch 00023: val_loss did not improve from 0.63761
3042/3042 - 133s - loss: 0.2953 - accuracy: 0.8939 - val_loss: 0.6926 - val_accuracy: 0.8260
Epoch 24/60

Epoch 00024: val_loss did not improve from 0.63761
3042/3042 - 134s - loss: 0.2909 - accuracy: 0.8950 - val_loss: 0.8001 - val_accuracy: 0.8256
Epoch 25/60

Epoch 00025: val_loss did not improve from 0.63761
3042/3042 - 133s - loss: 0.2886 - accuracy: 0.8959 - val_loss: 0.8133 - val_accuracy: 0.8145
Epoch 26/60

Epoch 00026: val_loss did not improve from 0.63761
3042/3042 - 133s - loss: 0.2852 - accuracy: 0.8980 - val_loss: 0.8134 - val_accuracy: 0.8128
Epoch 27/60

Epoch 00027: val_loss did not improve from 0.63761
3042/3042 - 133s - loss: 0.2806 - accuracy: 0.8984 - val_loss: 0.7568 - val_accuracy: 0.8220
Epoch 28/60

Epoch 00028: val_loss did not improve from 0.63761
3042/3042 - 134s - loss: 0.2782 - accuracy: 0.9005 - val_loss: 0.8254 - val_accuracy: 0.8219
Epoch 29/60

Epoch 00029: val_loss did not improve from 0.63761
3042/3042 - 134s - loss: 0.2746 - accuracy: 0.9015 - val_loss: 0.6839 - val_accuracy: 0.8356
Epoch 30/60

Epoch 00030: val_loss did not improve from 0.63761
3042/3042 - 133s - loss: 0.2729 - accuracy: 0.9025 - val_loss: 0.6950 - val_accuracy: 0.8256
Epoch 31/60

Epoch 00031: val_loss did not improve from 0.63761
3042/3042 - 132s - loss: 0.2700 - accuracy: 0.9039 - val_loss: 0.6890 - val_accuracy: 0.8413
Epoch 32/60

Epoch 00032: val_loss did not improve from 0.63761
3042/3042 - 133s - loss: 0.2658 - accuracy: 0.9052 - val_loss: 0.7852 - val_accuracy: 0.8341
Epoch 33/60

Epoch 00033: val_loss did not improve from 0.63761
3042/3042 - 128s - loss: 0.2634 - accuracy: 0.9056 - val_loss: 0.9108 - val_accuracy: 0.8316
Epoch 34/60

Epoch 00034: val_loss did not improve from 0.63761
3042/3042 - 129s - loss: 0.2610 - accuracy: 0.9069 - val_loss: 0.8498 - val_accuracy: 0.8249
Epoch 35/60

Epoch 00035: val_loss did not improve from 0.63761
3042/3042 - 129s - loss: 0.2583 - accuracy: 0.9079 - val_loss: 0.9585 - val_accuracy: 0.8122
Epoch 36/60

Epoch 00036: val_loss did not improve from 0.63761
3042/3042 - 130s - loss: 0.2566 - accuracy: 0.9083 - val_loss: 0.8261 - val_accuracy: 0.8392
Epoch 37/60

Epoch 00037: val_loss did not improve from 0.63761
3042/3042 - 131s - loss: 0.2529 - accuracy: 0.9098 - val_loss: 0.9568 - val_accuracy: 0.8228
Epoch 38/60

Epoch 00038: val_loss did not improve from 0.63761
3042/3042 - 131s - loss: 0.2532 - accuracy: 0.9098 - val_loss: 0.8472 - val_accuracy: 0.8306
Epoch 39/60

Epoch 00039: val_loss did not improve from 0.63761
3042/3042 - 132s - loss: 0.2505 - accuracy: 0.9119 - val_loss: 0.8338 - val_accuracy: 0.8427
Epoch 40/60

Epoch 00040: val_loss did not improve from 0.63761
3042/3042 - 129s - loss: 0.2478 - accuracy: 0.9119 - val_loss: 0.7749 - val_accuracy: 0.8414
Epoch 41/60

Epoch 00041: val_loss did not improve from 0.63761
3042/3042 - 130s - loss: 0.2458 - accuracy: 0.9126 - val_loss: 0.7579 - val_accuracy: 0.8301
Epoch 42/60

Epoch 00042: val_loss did not improve from 0.63761
3042/3042 - 128s - loss: 0.2431 - accuracy: 0.9144 - val_loss: 0.6853 - val_accuracy: 0.8323
Epoch 43/60

Epoch 00043: val_loss did not improve from 0.63761
3042/3042 - 129s - loss: 0.2411 - accuracy: 0.9152 - val_loss: 0.7859 - val_accuracy: 0.8336
Epoch 44/60

Epoch 00044: val_loss did not improve from 0.63761
3042/3042 - 129s - loss: 0.2417 - accuracy: 0.9143 - val_loss: 0.8734 - val_accuracy: 0.8206
Epoch 45/60

Epoch 00045: val_loss did not improve from 0.63761
3042/3042 - 130s - loss: 0.2383 - accuracy: 0.9157 - val_loss: 1.0692 - val_accuracy: 0.8081
Epoch 46/60

Epoch 00046: val_loss did not improve from 0.63761
3042/3042 - 131s - loss: 0.2364 - accuracy: 0.9164 - val_loss: 0.7515 - val_accuracy: 0.8432
Epoch 47/60

Epoch 00047: val_loss did not improve from 0.63761
3042/3042 - 131s - loss: 0.2339 - accuracy: 0.9173 - val_loss: 0.8454 - val_accuracy: 0.8347
Epoch 48/60

Epoch 00048: val_loss did not improve from 0.63761
3042/3042 - 130s - loss: 0.2329 - accuracy: 0.9179 - val_loss: 0.8935 - val_accuracy: 0.8234
Epoch 49/60

Epoch 00049: val_loss did not improve from 0.63761
3042/3042 - 129s - loss: 0.2327 - accuracy: 0.9183 - val_loss: 1.0099 - val_accuracy: 0.8299
Epoch 50/60

Epoch 00050: val_loss did not improve from 0.63761
3042/3042 - 129s - loss: 0.2289 - accuracy: 0.9196 - val_loss: 0.8685 - val_accuracy: 0.8366
Epoch 51/60

Epoch 00051: val_loss did not improve from 0.63761
3042/3042 - 132s - loss: 0.2288 - accuracy: 0.9196 - val_loss: 0.9036 - val_accuracy: 0.8400
Epoch 52/60

Epoch 00052: val_loss did not improve from 0.63761
3042/3042 - 130s - loss: 0.2276 - accuracy: 0.9211 - val_loss: 0.9233 - val_accuracy: 0.8317
Epoch 53/60

Epoch 00053: val_loss did not improve from 0.63761
3042/3042 - 129s - loss: 0.2258 - accuracy: 0.9208 - val_loss: 0.9700 - val_accuracy: 0.8272
Epoch 54/60

Epoch 00054: val_loss did not improve from 0.63761
3042/3042 - 129s - loss: 0.2241 - accuracy: 0.9215 - val_loss: 1.0450 - val_accuracy: 0.8312
Epoch 55/60

Epoch 00055: val_loss did not improve from 0.63761
3042/3042 - 131s - loss: 0.2219 - accuracy: 0.9220 - val_loss: 0.9944 - val_accuracy: 0.8371
Epoch 56/60

Epoch 00056: val_loss did not improve from 0.63761
3042/3042 - 132s - loss: 0.2213 - accuracy: 0.9227 - val_loss: 0.8221 - val_accuracy: 0.8429
Epoch 00056: early stopping
