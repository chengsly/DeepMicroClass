---------- Begin SLURM Prolog ----------
Job ID:        7322219
Username:      siliangc
Accountname:   lc_fs3
Name:          batchrun.sh
Partition:     main
Nodelist:      hpc4670
TasksPerNode:  1
CPUsPerTask:   Default[1]
TMPDIR:        /tmp/7322219.main
SCRATCHDIR:    /staging/scratch/7322219
Cluster:       uschpc
HSDA Account:  false
---------- 2020-05-14 01:21:45 ---------
2020-05-14 02:34:46.728404: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1
2020-05-14 02:34:46.852341: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: 
pciBusID: 0000:3b:00.0 name: Tesla V100-PCIE-32GB computeCapability: 7.0
coreClock: 1.38GHz coreCount: 80 deviceMemorySize: 31.72GiB deviceMemoryBandwidth: 836.37GiB/s
2020-05-14 02:34:46.861745: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 1 with properties: 
pciBusID: 0000:d8:00.0 name: Tesla V100-PCIE-32GB computeCapability: 7.0
coreClock: 1.38GHz coreCount: 80 deviceMemorySize: 31.72GiB deviceMemoryBandwidth: 836.37GiB/s
2020-05-14 02:34:46.863606: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
2020-05-14 02:34:46.870297: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
2020-05-14 02:34:46.874064: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10
2020-05-14 02:34:46.875461: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10
2020-05-14 02:34:46.880304: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10
2020-05-14 02:34:46.882678: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10
2020-05-14 02:34:46.889290: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-05-14 02:34:46.894338: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0, 1
2020-05-14 02:34:46.894738: I tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA
2020-05-14 02:34:46.904996: I tensorflow/core/platform/profile_utils/cpu_utils.cc:102] CPU Frequency: 2100000000 Hz
2020-05-14 02:34:46.907239: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x56452c38fd40 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-05-14 02:34:46.907262: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2020-05-14 02:34:47.208927: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x56452c3f9ef0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2020-05-14 02:34:47.208998: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-PCIE-32GB, Compute Capability 7.0
2020-05-14 02:34:47.209024: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (1): Tesla V100-PCIE-32GB, Compute Capability 7.0
2020-05-14 02:34:47.223169: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: 
pciBusID: 0000:3b:00.0 name: Tesla V100-PCIE-32GB computeCapability: 7.0
coreClock: 1.38GHz coreCount: 80 deviceMemorySize: 31.72GiB deviceMemoryBandwidth: 836.37GiB/s
2020-05-14 02:34:47.225490: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 1 with properties: 
pciBusID: 0000:d8:00.0 name: Tesla V100-PCIE-32GB computeCapability: 7.0
coreClock: 1.38GHz coreCount: 80 deviceMemorySize: 31.72GiB deviceMemoryBandwidth: 836.37GiB/s
2020-05-14 02:34:47.225547: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
2020-05-14 02:34:47.225572: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
2020-05-14 02:34:47.225599: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10
2020-05-14 02:34:47.225621: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10
2020-05-14 02:34:47.225642: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10
2020-05-14 02:34:47.225664: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10
2020-05-14 02:34:47.225685: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-05-14 02:34:47.241053: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0, 1
2020-05-14 02:34:47.241098: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
2020-05-14 02:34:47.244147: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-05-14 02:34:47.244162: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1108]      0 1 
2020-05-14 02:34:47.244170: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1121] 0:   N Y 
2020-05-14 02:34:47.244176: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1121] 1:   Y N 
2020-05-14 02:34:47.249190: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1247] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 30233 MB memory) -> physical GPU (device: 0, name: Tesla V100-PCIE-32GB, pci bus id: 0000:3b:00.0, compute capability: 7.0)
2020-05-14 02:34:47.251011: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1247] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 30233 MB memory) -> physical GPU (device: 1, name: Tesla V100-PCIE-32GB, pci bus id: 0000:d8:00.0, compute capability: 7.0)
2020-05-14 02:34:54.702280: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 14155200000 exceeds 10% of free system memory.
2020-05-14 02:35:04.836821: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 14155200000 exceeds 10% of free system memory.
2020-05-14 02:35:12.526042: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
2020-05-14 02:35:12.851194: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
Processing Data...
Processing sequences of length: 5kk

Processing forward host data...
Processing prokaryote training data from ProkTr#ProkTr#5k_seq39804_codefw.npy
Processing eukaryote training data from EukTr#EukTr#5k_seq38517_codefw.npy
Processing reverse host data...
Processing prokaryote training data from ProkTr#ProkTr#5k_seq39804_codebw.npy
Processing eukaryote training data from EukTr#EukTr#5k_seq38517_codebw.npy
Processing plasmid data
Processing plasmid data - bw
Processing virus data...
Processing prokaryotevirus training data from ProkVirusTr#ProkVirusTr#5k_seq39434_codefw.npy
Processin eukaryotevirus training data from EukVirusTr#EukVirusTr#5k_seq17340_codefw.npy
Processing virus data...
Processing prokaryotevirus training data from ProkVirusTr#ProkVirusTr#5k_seq39434_codebw.npy
Processin eukaryotevirus training data from EukVirusTr#EukVirusTr#5k_seq17340_codebw.npy
Processing Validation Data...
Processing Validation Data - bw...
(176940, 5000, 4)
(176940, 5000, 4)
(176940, 5)
(18899, 5000, 4)
(18899, 5000, 4)
(18899, 5)
Model: "model"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            [(None, None, 4)]    0                                            
__________________________________________________________________________________________________
input_2 (InputLayer)            [(None, None, 4)]    0                                            
__________________________________________________________________________________________________
conv1d (Conv1D)                 (None, None, 64)     1600        input_1[0][0]                    
                                                                 input_2[0][0]                    
__________________________________________________________________________________________________
max_pooling1d (MaxPooling1D)    (None, None, 64)     0           conv1d[0][0]                     
                                                                 conv1d[1][0]                     
__________________________________________________________________________________________________
batch_normalization (BatchNorma (None, None, 64)     256         max_pooling1d[0][0]              
                                                                 max_pooling1d[1][0]              
__________________________________________________________________________________________________
conv1d_1 (Conv1D)               (None, None, 128)    24704       batch_normalization[0][0]        
                                                                 batch_normalization[1][0]        
__________________________________________________________________________________________________
max_pooling1d_1 (MaxPooling1D)  (None, None, 128)    0           conv1d_1[0][0]                   
                                                                 conv1d_1[1][0]                   
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, None, 128)    512         max_pooling1d_1[0][0]            
                                                                 max_pooling1d_1[1][0]            
__________________________________________________________________________________________________
conv1d_2 (Conv1D)               (None, None, 256)    65792       batch_normalization_1[0][0]      
                                                                 batch_normalization_1[1][0]      
__________________________________________________________________________________________________
global_max_pooling1d (GlobalMax (None, 256)          0           conv1d_2[0][0]                   
                                                                 conv1d_2[1][0]                   
__________________________________________________________________________________________________
dropout (Dropout)               (None, 256)          0           global_max_pooling1d[0][0]       
                                                                 global_max_pooling1d[1][0]       
__________________________________________________________________________________________________
flatten (Flatten)               (None, 256)          0           dropout[0][0]                    
                                                                 dropout[1][0]                    
__________________________________________________________________________________________________
dense (Dense)                   (None, 500)          128500      flatten[0][0]                    
                                                                 flatten[1][0]                    
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 500)          0           dense[0][0]                      
                                                                 dense[1][0]                      
__________________________________________________________________________________________________
dense_1 (Dense)                 (None, 5)            2505        dropout_1[0][0]                  
                                                                 dropout_1[1][0]                  
__________________________________________________________________________________________________
average (Average)               (None, 5)            0           dense_1[0][0]                    
                                                                 dense_1[1][0]                    
==================================================================================================
Total params: 223,869
Trainable params: 223,485
Non-trainable params: 384
__________________________________________________________________________________________________
Epoch 1/60

Epoch 00001: val_loss improved from inf to 0.77259, saving model to ./models/model_DNA_one-hot_5000.h5
1770/1770 - 159s - loss: 0.9671 - accuracy: 0.5991 - val_loss: 0.7726 - val_accuracy: 0.7216
Epoch 2/60

Epoch 00002: val_loss improved from 0.77259 to 0.62456, saving model to ./models/model_DNA_one-hot_5000.h5
1770/1770 - 158s - loss: 0.6383 - accuracy: 0.7400 - val_loss: 0.6246 - val_accuracy: 0.7487
Epoch 3/60

Epoch 00003: val_loss did not improve from 0.62456
1770/1770 - 158s - loss: 0.5113 - accuracy: 0.8028 - val_loss: 0.6660 - val_accuracy: 0.7476
Epoch 4/60

Epoch 00004: val_loss did not improve from 0.62456
1770/1770 - 161s - loss: 0.4523 - accuracy: 0.8287 - val_loss: 0.6771 - val_accuracy: 0.7936
Epoch 5/60

Epoch 00005: val_loss did not improve from 0.62456
1770/1770 - 158s - loss: 0.4148 - accuracy: 0.8453 - val_loss: 0.6354 - val_accuracy: 0.8039
Epoch 6/60

Epoch 00006: val_loss did not improve from 0.62456
1770/1770 - 160s - loss: 0.3911 - accuracy: 0.8559 - val_loss: 0.6334 - val_accuracy: 0.8225
Epoch 7/60

Epoch 00007: val_loss did not improve from 0.62456
1770/1770 - 158s - loss: 0.3720 - accuracy: 0.8640 - val_loss: 0.7726 - val_accuracy: 0.8279
Epoch 8/60

Epoch 00008: val_loss did not improve from 0.62456
1770/1770 - 158s - loss: 0.3568 - accuracy: 0.8695 - val_loss: 0.8405 - val_accuracy: 0.8084
Epoch 9/60

Epoch 00009: val_loss did not improve from 0.62456
1770/1770 - 159s - loss: 0.3426 - accuracy: 0.8761 - val_loss: 0.6424 - val_accuracy: 0.8279
Epoch 10/60

Epoch 00010: val_loss did not improve from 0.62456
1770/1770 - 158s - loss: 0.3310 - accuracy: 0.8805 - val_loss: 0.6958 - val_accuracy: 0.8338
Epoch 11/60

Epoch 00011: val_loss did not improve from 0.62456
1770/1770 - 158s - loss: 0.3227 - accuracy: 0.8843 - val_loss: 0.6734 - val_accuracy: 0.8335
Epoch 12/60

Epoch 00012: val_loss did not improve from 0.62456
1770/1770 - 158s - loss: 0.3124 - accuracy: 0.8878 - val_loss: 0.8304 - val_accuracy: 0.8283
Epoch 13/60

Epoch 00013: val_loss did not improve from 0.62456
1770/1770 - 157s - loss: 0.3032 - accuracy: 0.8922 - val_loss: 0.8760 - val_accuracy: 0.8302
Epoch 14/60

Epoch 00014: val_loss did not improve from 0.62456
1770/1770 - 157s - loss: 0.2957 - accuracy: 0.8951 - val_loss: 0.7840 - val_accuracy: 0.8386
Epoch 15/60

Epoch 00015: val_loss did not improve from 0.62456
1770/1770 - 157s - loss: 0.2879 - accuracy: 0.8980 - val_loss: 0.6335 - val_accuracy: 0.8350
Epoch 16/60

Epoch 00016: val_loss did not improve from 0.62456
1770/1770 - 158s - loss: 0.2827 - accuracy: 0.9004 - val_loss: 0.8170 - val_accuracy: 0.8329
Epoch 17/60

Epoch 00017: val_loss did not improve from 0.62456
1770/1770 - 158s - loss: 0.2757 - accuracy: 0.9024 - val_loss: 0.7636 - val_accuracy: 0.8429
Epoch 18/60

Epoch 00018: val_loss did not improve from 0.62456
1770/1770 - 158s - loss: 0.2722 - accuracy: 0.9036 - val_loss: 0.8172 - val_accuracy: 0.8482
Epoch 19/60

Epoch 00019: val_loss did not improve from 0.62456
1770/1770 - 158s - loss: 0.2683 - accuracy: 0.9055 - val_loss: 0.8681 - val_accuracy: 0.8411
Epoch 20/60

Epoch 00020: val_loss did not improve from 0.62456
1770/1770 - 158s - loss: 0.2630 - accuracy: 0.9076 - val_loss: 0.7286 - val_accuracy: 0.8496
Epoch 21/60

Epoch 00021: val_loss did not improve from 0.62456
1770/1770 - 158s - loss: 0.2571 - accuracy: 0.9097 - val_loss: 0.8695 - val_accuracy: 0.8458
Epoch 22/60

Epoch 00022: val_loss did not improve from 0.62456
1770/1770 - 157s - loss: 0.2525 - accuracy: 0.9122 - val_loss: 0.7933 - val_accuracy: 0.8470
Epoch 23/60

Epoch 00023: val_loss did not improve from 0.62456
1770/1770 - 158s - loss: 0.2480 - accuracy: 0.9139 - val_loss: 0.9703 - val_accuracy: 0.8423
Epoch 24/60

Epoch 00024: val_loss did not improve from 0.62456
1770/1770 - 161s - loss: 0.2454 - accuracy: 0.9148 - val_loss: 0.9196 - val_accuracy: 0.8450
Epoch 25/60

Epoch 00025: val_loss did not improve from 0.62456
1770/1770 - 157s - loss: 0.2424 - accuracy: 0.9168 - val_loss: 0.8547 - val_accuracy: 0.8493
Epoch 26/60

Epoch 00026: val_loss did not improve from 0.62456
1770/1770 - 158s - loss: 0.2366 - accuracy: 0.9173 - val_loss: 0.8866 - val_accuracy: 0.8453
Epoch 27/60

Epoch 00027: val_loss did not improve from 0.62456
1770/1770 - 158s - loss: 0.2357 - accuracy: 0.9180 - val_loss: 0.9815 - val_accuracy: 0.8468
Epoch 28/60

Epoch 00028: val_loss did not improve from 0.62456
1770/1770 - 157s - loss: 0.2319 - accuracy: 0.9198 - val_loss: 0.8547 - val_accuracy: 0.8452
Epoch 29/60

Epoch 00029: val_loss did not improve from 0.62456
1770/1770 - 157s - loss: 0.2283 - accuracy: 0.9214 - val_loss: 1.0512 - val_accuracy: 0.8388
Epoch 30/60

Epoch 00030: val_loss did not improve from 0.62456
1770/1770 - 157s - loss: 0.2271 - accuracy: 0.9222 - val_loss: 0.9564 - val_accuracy: 0.8528
Epoch 31/60

Epoch 00031: val_loss did not improve from 0.62456
1770/1770 - 157s - loss: 0.2235 - accuracy: 0.9224 - val_loss: 0.9936 - val_accuracy: 0.8437
Epoch 32/60

Epoch 00032: val_loss did not improve from 0.62456
1770/1770 - 161s - loss: 0.2210 - accuracy: 0.9240 - val_loss: 1.0039 - val_accuracy: 0.8458
Epoch 33/60

Epoch 00033: val_loss did not improve from 0.62456
1770/1770 - 157s - loss: 0.2209 - accuracy: 0.9243 - val_loss: 0.9816 - val_accuracy: 0.8481
Epoch 34/60

Epoch 00034: val_loss did not improve from 0.62456
1770/1770 - 158s - loss: 0.2191 - accuracy: 0.9247 - val_loss: 1.0143 - val_accuracy: 0.8489
Epoch 35/60

Epoch 00035: val_loss did not improve from 0.62456
1770/1770 - 157s - loss: 0.2146 - accuracy: 0.9263 - val_loss: 0.9683 - val_accuracy: 0.8518
Epoch 36/60

Epoch 00036: val_loss did not improve from 0.62456
1770/1770 - 158s - loss: 0.2152 - accuracy: 0.9266 - val_loss: 0.9124 - val_accuracy: 0.8514
Epoch 37/60

Epoch 00037: val_loss did not improve from 0.62456
1770/1770 - 157s - loss: 0.2125 - accuracy: 0.9278 - val_loss: 0.9900 - val_accuracy: 0.8540
Epoch 38/60

Epoch 00038: val_loss did not improve from 0.62456
1770/1770 - 157s - loss: 0.2085 - accuracy: 0.9287 - val_loss: 0.9459 - val_accuracy: 0.8528
Epoch 39/60

Epoch 00039: val_loss did not improve from 0.62456
1770/1770 - 157s - loss: 0.2056 - accuracy: 0.9297 - val_loss: 0.8305 - val_accuracy: 0.8533
Epoch 40/60

Epoch 00040: val_loss did not improve from 0.62456
1770/1770 - 158s - loss: 0.2035 - accuracy: 0.9309 - val_loss: 1.0096 - val_accuracy: 0.8527
Epoch 41/60

Epoch 00041: val_loss did not improve from 0.62456
1770/1770 - 158s - loss: 0.2041 - accuracy: 0.9307 - val_loss: 1.0395 - val_accuracy: 0.8458
Epoch 42/60

Epoch 00042: val_loss did not improve from 0.62456
1770/1770 - 157s - loss: 0.2022 - accuracy: 0.9309 - val_loss: 1.0516 - val_accuracy: 0.8506
Epoch 43/60

Epoch 00043: val_loss did not improve from 0.62456
1770/1770 - 157s - loss: 0.1985 - accuracy: 0.9329 - val_loss: 1.0908 - val_accuracy: 0.8515
Epoch 44/60

Epoch 00044: val_loss did not improve from 0.62456
1770/1770 - 157s - loss: 0.1983 - accuracy: 0.9323 - val_loss: 0.9989 - val_accuracy: 0.8551
Epoch 45/60

Epoch 00045: val_loss did not improve from 0.62456
1770/1770 - 157s - loss: 0.1960 - accuracy: 0.9341 - val_loss: 1.0015 - val_accuracy: 0.8530
Epoch 46/60

Epoch 00046: val_loss did not improve from 0.62456
1770/1770 - 157s - loss: 0.1955 - accuracy: 0.9343 - val_loss: 1.0807 - val_accuracy: 0.8491
Epoch 47/60

Epoch 00047: val_loss did not improve from 0.62456
1770/1770 - 157s - loss: 0.1953 - accuracy: 0.9333 - val_loss: 1.0142 - val_accuracy: 0.8562
Epoch 48/60

Epoch 00048: val_loss did not improve from 0.62456
1770/1770 - 158s - loss: 0.1917 - accuracy: 0.9354 - val_loss: 0.9184 - val_accuracy: 0.8574
Epoch 49/60

Epoch 00049: val_loss did not improve from 0.62456
1770/1770 - 158s - loss: 0.1924 - accuracy: 0.9353 - val_loss: 0.9796 - val_accuracy: 0.8591
Epoch 50/60

Epoch 00050: val_loss did not improve from 0.62456
1770/1770 - 157s - loss: 0.1887 - accuracy: 0.9358 - val_loss: 0.9854 - val_accuracy: 0.8536
Epoch 51/60

Epoch 00051: val_loss did not improve from 0.62456
1770/1770 - 157s - loss: 0.1892 - accuracy: 0.9379 - val_loss: 1.1812 - val_accuracy: 0.8476
Epoch 52/60

Epoch 00052: val_loss did not improve from 0.62456
1770/1770 - 157s - loss: 0.1839 - accuracy: 0.9382 - val_loss: 1.0904 - val_accuracy: 0.8549
Epoch 53/60

Epoch 00053: val_loss did not improve from 0.62456
1770/1770 - 157s - loss: 0.1846 - accuracy: 0.9379 - val_loss: 1.0973 - val_accuracy: 0.8499
Epoch 54/60

Epoch 00054: val_loss did not improve from 0.62456
1770/1770 - 158s - loss: 0.1858 - accuracy: 0.9381 - val_loss: 0.9690 - val_accuracy: 0.8548
Epoch 55/60

Epoch 00055: val_loss did not improve from 0.62456
1770/1770 - 158s - loss: 0.1868 - accuracy: 0.9379 - val_loss: 1.0706 - val_accuracy: 0.8490
Epoch 56/60

Epoch 00056: val_loss did not improve from 0.62456
1770/1770 - 158s - loss: 0.1803 - accuracy: 0.9389 - val_loss: 0.9930 - val_accuracy: 0.8590
Epoch 57/60

Epoch 00057: val_loss did not improve from 0.62456
1770/1770 - 158s - loss: 0.1821 - accuracy: 0.9397 - val_loss: 1.0530 - val_accuracy: 0.8507
Epoch 58/60

Epoch 00058: val_loss did not improve from 0.62456
1770/1770 - 158s - loss: 0.1801 - accuracy: 0.9402 - val_loss: 1.0823 - val_accuracy: 0.8576
Epoch 59/60

Epoch 00059: val_loss did not improve from 0.62456
1770/1770 - 158s - loss: 0.1789 - accuracy: 0.9404 - val_loss: 1.1564 - val_accuracy: 0.8495
Epoch 00059: early stopping
