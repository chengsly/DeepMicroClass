---------- Begin SLURM Prolog ----------
Job ID:        7322218
Username:      siliangc
Accountname:   lc_fs3
Name:          batchrun.sh
Partition:     main
Nodelist:      hpc4668
TasksPerNode:  1
CPUsPerTask:   Default[1]
TMPDIR:        /tmp/7322218.main
SCRATCHDIR:    /staging/scratch/7322218
Cluster:       uschpc
HSDA Account:  false
---------- 2020-05-13 23:35:45 ---------
2020-05-14 00:52:39.509448: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1
2020-05-14 00:52:39.607187: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: 
pciBusID: 0000:3b:00.0 name: Tesla V100-PCIE-32GB computeCapability: 7.0
coreClock: 1.38GHz coreCount: 80 deviceMemorySize: 31.72GiB deviceMemoryBandwidth: 836.37GiB/s
2020-05-14 00:52:39.610906: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 1 with properties: 
pciBusID: 0000:d8:00.0 name: Tesla V100-PCIE-32GB computeCapability: 7.0
coreClock: 1.38GHz coreCount: 80 deviceMemorySize: 31.72GiB deviceMemoryBandwidth: 836.37GiB/s
2020-05-14 00:52:39.619650: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
2020-05-14 00:52:40.271906: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
2020-05-14 00:52:40.426688: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10
2020-05-14 00:52:40.818070: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10
2020-05-14 00:52:41.356355: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10
2020-05-14 00:52:41.638621: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10
2020-05-14 00:52:42.349070: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-05-14 00:52:42.362907: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0, 1
2020-05-14 00:52:42.363504: I tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA
2020-05-14 00:52:42.377701: I tensorflow/core/platform/profile_utils/cpu_utils.cc:102] CPU Frequency: 2100000000 Hz
2020-05-14 00:52:42.380339: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x56229ad321e0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:
2020-05-14 00:52:42.380368: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version
2020-05-14 00:52:42.771427: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x56229ad9c390 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:
2020-05-14 00:52:42.771508: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla V100-PCIE-32GB, Compute Capability 7.0
2020-05-14 00:52:42.771537: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (1): Tesla V100-PCIE-32GB, Compute Capability 7.0
2020-05-14 00:52:42.782135: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 0 with properties: 
pciBusID: 0000:3b:00.0 name: Tesla V100-PCIE-32GB computeCapability: 7.0
coreClock: 1.38GHz coreCount: 80 deviceMemorySize: 31.72GiB deviceMemoryBandwidth: 836.37GiB/s
2020-05-14 00:52:42.784588: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1561] Found device 1 with properties: 
pciBusID: 0000:d8:00.0 name: Tesla V100-PCIE-32GB computeCapability: 7.0
coreClock: 1.38GHz coreCount: 80 deviceMemorySize: 31.72GiB deviceMemoryBandwidth: 836.37GiB/s
2020-05-14 00:52:42.784650: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
2020-05-14 00:52:42.784676: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
2020-05-14 00:52:42.784705: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10
2020-05-14 00:52:42.784729: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10
2020-05-14 00:52:42.784752: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10
2020-05-14 00:52:42.784774: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10
2020-05-14 00:52:42.784798: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
2020-05-14 00:52:42.791762: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1703] Adding visible gpu devices: 0, 1
2020-05-14 00:52:42.791800: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1
2020-05-14 00:52:42.799103: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1102] Device interconnect StreamExecutor with strength 1 edge matrix:
2020-05-14 00:52:42.799121: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1108]      0 1 
2020-05-14 00:52:42.799130: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1121] 0:   N Y 
2020-05-14 00:52:42.799137: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1121] 1:   Y N 
2020-05-14 00:52:42.804697: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1247] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 30233 MB memory) -> physical GPU (device: 0, name: Tesla V100-PCIE-32GB, pci bus id: 0000:3b:00.0, compute capability: 7.0)
2020-05-14 00:52:42.806654: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1247] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 30233 MB memory) -> physical GPU (device: 1, name: Tesla V100-PCIE-32GB, pci bus id: 0000:d8:00.0, compute capability: 7.0)
2020-05-14 00:52:57.574160: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 14599008000 exceeds 10% of free system memory.
2020-05-14 00:53:12.738660: W tensorflow/core/framework/cpu_allocator_impl.cc:81] Allocation of 14599008000 exceeds 10% of free system memory.
2020-05-14 00:53:30.107247: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10
2020-05-14 00:53:31.156031: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7
Processing Data...
Processing sequences of length: 3kk

Processing forward host data...
Processing prokaryote training data from ProkTr#ProkTr#3k_num1_seq66847_codefw.npy
Processing eukaryote training data from EukTr#EukTr#3k_num1_seq65382_codefw.npy
Processing reverse host data...
Processing prokaryote training data from ProkTr#ProkTr#3k_num1_seq66847_codebw.npy
Processing eukaryote training data from EukTr#EukTr#3k_num1_seq65382_codebw.npy
Processing plasmid data
Processing plasmid data - bw
Processing virus data...
Processing prokaryotevirus training data from ProkVirusTr#ProkVirusTr#3k_num1_seq66765_codefw.npy
Processin eukaryotevirus training data from EukVirusTr#EukVirusTr#3k_num1_seq34669_codefw.npy
Processing virus data...
Processing prokaryotevirus training data from ProkVirusTr#ProkVirusTr#3k_num1_seq66765_codebw.npy
Processin eukaryotevirus training data from EukVirusTr#EukVirusTr#3k_num1_seq34669_codebw.npy
Processing Validation Data...
Processing Validation Data - bw...
(304146, 3000, 4)
(304146, 3000, 4)
(304146, 5)
(32442, 3000, 4)
(32442, 3000, 4)
(32442, 5)
Model: "model"
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            [(None, None, 4)]    0                                            
__________________________________________________________________________________________________
input_2 (InputLayer)            [(None, None, 4)]    0                                            
__________________________________________________________________________________________________
conv1d (Conv1D)                 (None, None, 64)     1600        input_1[0][0]                    
                                                                 input_2[0][0]                    
__________________________________________________________________________________________________
max_pooling1d (MaxPooling1D)    (None, None, 64)     0           conv1d[0][0]                     
                                                                 conv1d[1][0]                     
__________________________________________________________________________________________________
batch_normalization (BatchNorma (None, None, 64)     256         max_pooling1d[0][0]              
                                                                 max_pooling1d[1][0]              
__________________________________________________________________________________________________
conv1d_1 (Conv1D)               (None, None, 128)    24704       batch_normalization[0][0]        
                                                                 batch_normalization[1][0]        
__________________________________________________________________________________________________
max_pooling1d_1 (MaxPooling1D)  (None, None, 128)    0           conv1d_1[0][0]                   
                                                                 conv1d_1[1][0]                   
__________________________________________________________________________________________________
batch_normalization_1 (BatchNor (None, None, 128)    512         max_pooling1d_1[0][0]            
                                                                 max_pooling1d_1[1][0]            
__________________________________________________________________________________________________
conv1d_2 (Conv1D)               (None, None, 256)    65792       batch_normalization_1[0][0]      
                                                                 batch_normalization_1[1][0]      
__________________________________________________________________________________________________
global_max_pooling1d (GlobalMax (None, 256)          0           conv1d_2[0][0]                   
                                                                 conv1d_2[1][0]                   
__________________________________________________________________________________________________
dropout (Dropout)               (None, 256)          0           global_max_pooling1d[0][0]       
                                                                 global_max_pooling1d[1][0]       
__________________________________________________________________________________________________
flatten (Flatten)               (None, 256)          0           dropout[0][0]                    
                                                                 dropout[1][0]                    
__________________________________________________________________________________________________
dense (Dense)                   (None, 500)          128500      flatten[0][0]                    
                                                                 flatten[1][0]                    
__________________________________________________________________________________________________
dropout_1 (Dropout)             (None, 500)          0           dense[0][0]                      
                                                                 dense[1][0]                      
__________________________________________________________________________________________________
dense_1 (Dense)                 (None, 5)            2505        dropout_1[0][0]                  
                                                                 dropout_1[1][0]                  
__________________________________________________________________________________________________
average (Average)               (None, 5)            0           dense_1[0][0]                    
                                                                 dense_1[1][0]                    
==================================================================================================
Total params: 223,869
Trainable params: 223,485
Non-trainable params: 384
__________________________________________________________________________________________________
Epoch 1/60

Epoch 00001: val_loss improved from inf to 0.86938, saving model to ./models/model_DNA_one-hot_3000.h5
3042/3042 - 165s - loss: 0.8912 - accuracy: 0.6302 - val_loss: 0.8694 - val_accuracy: 0.6767
Epoch 2/60

Epoch 00002: val_loss improved from 0.86938 to 0.73037, saving model to ./models/model_DNA_one-hot_3000.h5
3042/3042 - 164s - loss: 0.5979 - accuracy: 0.7632 - val_loss: 0.7304 - val_accuracy: 0.7708
Epoch 3/60

Epoch 00003: val_loss did not improve from 0.73037
3042/3042 - 165s - loss: 0.5173 - accuracy: 0.8030 - val_loss: 0.8374 - val_accuracy: 0.7800
Epoch 4/60

Epoch 00004: val_loss did not improve from 0.73037
3042/3042 - 164s - loss: 0.4748 - accuracy: 0.8215 - val_loss: 0.8306 - val_accuracy: 0.7809
Epoch 5/60

Epoch 00005: val_loss improved from 0.73037 to 0.70153, saving model to ./models/model_DNA_one-hot_3000.h5
3042/3042 - 165s - loss: 0.4495 - accuracy: 0.8326 - val_loss: 0.7015 - val_accuracy: 0.8009
Epoch 6/60

Epoch 00006: val_loss did not improve from 0.70153
3042/3042 - 165s - loss: 0.4306 - accuracy: 0.8406 - val_loss: 0.8001 - val_accuracy: 0.7985
Epoch 7/60

Epoch 00007: val_loss improved from 0.70153 to 0.68026, saving model to ./models/model_DNA_one-hot_3000.h5
3042/3042 - 165s - loss: 0.4138 - accuracy: 0.8469 - val_loss: 0.6803 - val_accuracy: 0.8035
Epoch 8/60

Epoch 00008: val_loss did not improve from 0.68026
3042/3042 - 165s - loss: 0.4005 - accuracy: 0.8532 - val_loss: 0.6902 - val_accuracy: 0.7987
Epoch 9/60

Epoch 00009: val_loss did not improve from 0.68026
3042/3042 - 165s - loss: 0.3879 - accuracy: 0.8588 - val_loss: 0.7589 - val_accuracy: 0.8084
Epoch 10/60

Epoch 00010: val_loss did not improve from 0.68026
3042/3042 - 165s - loss: 0.3781 - accuracy: 0.8620 - val_loss: 0.7223 - val_accuracy: 0.8137
Epoch 11/60

Epoch 00011: val_loss did not improve from 0.68026
3042/3042 - 165s - loss: 0.3679 - accuracy: 0.8659 - val_loss: 0.7468 - val_accuracy: 0.8172
Epoch 12/60

Epoch 00012: val_loss did not improve from 0.68026
3042/3042 - 165s - loss: 0.3610 - accuracy: 0.8690 - val_loss: 0.7450 - val_accuracy: 0.8199
Epoch 13/60

Epoch 00013: val_loss did not improve from 0.68026
3042/3042 - 165s - loss: 0.3531 - accuracy: 0.8722 - val_loss: 0.7806 - val_accuracy: 0.8200
Epoch 14/60

Epoch 00014: val_loss did not improve from 0.68026
3042/3042 - 165s - loss: 0.3474 - accuracy: 0.8745 - val_loss: 0.7586 - val_accuracy: 0.8290
Epoch 15/60

Epoch 00015: val_loss did not improve from 0.68026
3042/3042 - 165s - loss: 0.3412 - accuracy: 0.8771 - val_loss: 0.7367 - val_accuracy: 0.8324
Epoch 16/60

Epoch 00016: val_loss did not improve from 0.68026
3042/3042 - 165s - loss: 0.3345 - accuracy: 0.8801 - val_loss: 0.7389 - val_accuracy: 0.8300
Epoch 17/60

Epoch 00017: val_loss did not improve from 0.68026
3042/3042 - 165s - loss: 0.3300 - accuracy: 0.8812 - val_loss: 0.7279 - val_accuracy: 0.8237
Epoch 18/60

Epoch 00018: val_loss improved from 0.68026 to 0.63411, saving model to ./models/model_DNA_one-hot_3000.h5
3042/3042 - 165s - loss: 0.3257 - accuracy: 0.8829 - val_loss: 0.6341 - val_accuracy: 0.8292
Epoch 19/60

Epoch 00019: val_loss did not improve from 0.63411
3042/3042 - 165s - loss: 0.3208 - accuracy: 0.8856 - val_loss: 0.7532 - val_accuracy: 0.8330
Epoch 20/60

Epoch 00020: val_loss did not improve from 0.63411
3042/3042 - 165s - loss: 0.3172 - accuracy: 0.8874 - val_loss: 0.7230 - val_accuracy: 0.8192
Epoch 21/60

Epoch 00021: val_loss did not improve from 0.63411
3042/3042 - 165s - loss: 0.3110 - accuracy: 0.8893 - val_loss: 0.8976 - val_accuracy: 0.8278
Epoch 22/60

Epoch 00022: val_loss did not improve from 0.63411
3042/3042 - 165s - loss: 0.3104 - accuracy: 0.8891 - val_loss: 0.6457 - val_accuracy: 0.8349
Epoch 23/60

Epoch 00023: val_loss did not improve from 0.63411
3042/3042 - 165s - loss: 0.3067 - accuracy: 0.8907 - val_loss: 0.7778 - val_accuracy: 0.8249
Epoch 24/60

Epoch 00024: val_loss did not improve from 0.63411
3042/3042 - 164s - loss: 0.3017 - accuracy: 0.8923 - val_loss: 0.8140 - val_accuracy: 0.8261
Epoch 25/60

Epoch 00025: val_loss did not improve from 0.63411
3042/3042 - 164s - loss: 0.2995 - accuracy: 0.8938 - val_loss: 0.8162 - val_accuracy: 0.8308
Epoch 26/60

Epoch 00026: val_loss did not improve from 0.63411
3042/3042 - 164s - loss: 0.2950 - accuracy: 0.8949 - val_loss: 0.9691 - val_accuracy: 0.8204
Epoch 27/60

Epoch 00027: val_loss did not improve from 0.63411
3042/3042 - 165s - loss: 0.2934 - accuracy: 0.8959 - val_loss: 0.7666 - val_accuracy: 0.8336
Epoch 28/60

Epoch 00028: val_loss did not improve from 0.63411
3042/3042 - 164s - loss: 0.2882 - accuracy: 0.8979 - val_loss: 0.9536 - val_accuracy: 0.8312
Epoch 29/60

Epoch 00029: val_loss did not improve from 0.63411
3042/3042 - 164s - loss: 0.2861 - accuracy: 0.8990 - val_loss: 0.7595 - val_accuracy: 0.8391
Epoch 30/60

Epoch 00030: val_loss did not improve from 0.63411
3042/3042 - 164s - loss: 0.2850 - accuracy: 0.8992 - val_loss: 0.7381 - val_accuracy: 0.8406
Epoch 31/60

Epoch 00031: val_loss did not improve from 0.63411
3042/3042 - 165s - loss: 0.2814 - accuracy: 0.9007 - val_loss: 0.8045 - val_accuracy: 0.8307
Epoch 32/60

Epoch 00032: val_loss did not improve from 0.63411
3042/3042 - 165s - loss: 0.2795 - accuracy: 0.9015 - val_loss: 0.8031 - val_accuracy: 0.8358
Epoch 33/60

Epoch 00033: val_loss did not improve from 0.63411
3042/3042 - 165s - loss: 0.2771 - accuracy: 0.9022 - val_loss: 0.9463 - val_accuracy: 0.8316
Epoch 34/60

Epoch 00034: val_loss did not improve from 0.63411
3042/3042 - 165s - loss: 0.2758 - accuracy: 0.9035 - val_loss: 0.7710 - val_accuracy: 0.8385
Epoch 35/60

Epoch 00035: val_loss did not improve from 0.63411
3042/3042 - 165s - loss: 0.2724 - accuracy: 0.9043 - val_loss: 1.0180 - val_accuracy: 0.8329
Epoch 36/60

Epoch 00036: val_loss did not improve from 0.63411
3042/3042 - 165s - loss: 0.2714 - accuracy: 0.9047 - val_loss: 0.8087 - val_accuracy: 0.8288
Epoch 37/60

Epoch 00037: val_loss did not improve from 0.63411
3042/3042 - 165s - loss: 0.2690 - accuracy: 0.9058 - val_loss: 0.9907 - val_accuracy: 0.8380
Epoch 38/60

Epoch 00038: val_loss did not improve from 0.63411
3042/3042 - 165s - loss: 0.2676 - accuracy: 0.9054 - val_loss: 0.9698 - val_accuracy: 0.8355
Epoch 39/60

Epoch 00039: val_loss did not improve from 0.63411
3042/3042 - 165s - loss: 0.2663 - accuracy: 0.9061 - val_loss: 0.8170 - val_accuracy: 0.8375
Epoch 40/60

Epoch 00040: val_loss did not improve from 0.63411
3042/3042 - 165s - loss: 0.2632 - accuracy: 0.9078 - val_loss: 0.7906 - val_accuracy: 0.8421
Epoch 41/60

Epoch 00041: val_loss did not improve from 0.63411
3042/3042 - 165s - loss: 0.2611 - accuracy: 0.9089 - val_loss: 0.8752 - val_accuracy: 0.8434
Epoch 42/60

Epoch 00042: val_loss did not improve from 0.63411
3042/3042 - 165s - loss: 0.2582 - accuracy: 0.9098 - val_loss: 0.9925 - val_accuracy: 0.8365
Epoch 43/60

Epoch 00043: val_loss did not improve from 0.63411
3042/3042 - 165s - loss: 0.2579 - accuracy: 0.9099 - val_loss: 0.8446 - val_accuracy: 0.8389
Epoch 44/60

Epoch 00044: val_loss did not improve from 0.63411
3042/3042 - 165s - loss: 0.2570 - accuracy: 0.9103 - val_loss: 0.7538 - val_accuracy: 0.8457
Epoch 45/60

Epoch 00045: val_loss did not improve from 0.63411
3042/3042 - 165s - loss: 0.2530 - accuracy: 0.9120 - val_loss: 0.8411 - val_accuracy: 0.8393
Epoch 46/60

Epoch 00046: val_loss did not improve from 0.63411
3042/3042 - 164s - loss: 0.2533 - accuracy: 0.9116 - val_loss: 1.0929 - val_accuracy: 0.8320
Epoch 47/60

Epoch 00047: val_loss did not improve from 0.63411
3042/3042 - 166s - loss: 0.2515 - accuracy: 0.9121 - val_loss: 0.7806 - val_accuracy: 0.8458
Epoch 48/60

Epoch 00048: val_loss did not improve from 0.63411
3042/3042 - 165s - loss: 0.2495 - accuracy: 0.9132 - val_loss: 0.9969 - val_accuracy: 0.8403
Epoch 49/60

Epoch 00049: val_loss did not improve from 0.63411
3042/3042 - 165s - loss: 0.2484 - accuracy: 0.9137 - val_loss: 0.8155 - val_accuracy: 0.8408
Epoch 50/60

Epoch 00050: val_loss did not improve from 0.63411
3042/3042 - 165s - loss: 0.2471 - accuracy: 0.9137 - val_loss: 0.9417 - val_accuracy: 0.8422
Epoch 51/60

Epoch 00051: val_loss did not improve from 0.63411
3042/3042 - 165s - loss: 0.2469 - accuracy: 0.9140 - val_loss: 0.9025 - val_accuracy: 0.8397
Epoch 52/60

Epoch 00052: val_loss did not improve from 0.63411
3042/3042 - 165s - loss: 0.2441 - accuracy: 0.9150 - val_loss: 1.0379 - val_accuracy: 0.8337
Epoch 53/60

Epoch 00053: val_loss did not improve from 0.63411
3042/3042 - 165s - loss: 0.2445 - accuracy: 0.9155 - val_loss: 0.9731 - val_accuracy: 0.8378
Epoch 54/60

Epoch 00054: val_loss did not improve from 0.63411
3042/3042 - 165s - loss: 0.2414 - accuracy: 0.9157 - val_loss: 0.7794 - val_accuracy: 0.8481
Epoch 55/60

Epoch 00055: val_loss did not improve from 0.63411
3042/3042 - 165s - loss: 0.2404 - accuracy: 0.9162 - val_loss: 0.9652 - val_accuracy: 0.8370
Epoch 56/60

Epoch 00056: val_loss did not improve from 0.63411
3042/3042 - 164s - loss: 0.2383 - accuracy: 0.9166 - val_loss: 0.9540 - val_accuracy: 0.8430
Epoch 57/60

Epoch 00057: val_loss did not improve from 0.63411
3042/3042 - 165s - loss: 0.2388 - accuracy: 0.9174 - val_loss: 0.9528 - val_accuracy: 0.8382
Epoch 58/60

Epoch 00058: val_loss did not improve from 0.63411
3042/3042 - 164s - loss: 0.2402 - accuracy: 0.9171 - val_loss: 0.8304 - val_accuracy: 0.8452
Epoch 59/60

Epoch 00059: val_loss did not improve from 0.63411
3042/3042 - 165s - loss: 0.2339 - accuracy: 0.9190 - val_loss: 0.9923 - val_accuracy: 0.8406
Epoch 60/60

Epoch 00060: val_loss did not improve from 0.63411
3042/3042 - 164s - loss: 0.2350 - accuracy: 0.9185 - val_loss: 0.9506 - val_accuracy: 0.8424
